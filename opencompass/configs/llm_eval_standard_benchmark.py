from mmengine.config import read_base
from opencompass.models.openai_api import OpenAISDK

with read_base():
    from .lark import lark_bot_url
    # ç›´æ¥ä»é¢„è®¾çš„æ•°æ®é›†é…ç½®ä¸­è¯»å–æ‰€éœ€çš„æ•°æ®é›†é…ç½®
    # from .datasets.aime2025.aime2025_llmjudge_gen_5e9f4f import aime2025_datasets
    from .datasets.livecodebench.livecodebench_gen import LCB_datasets
    from .datasets.mmlu_pro.mmlu_pro_gen import mmlu_pro_datasets
    from .datasets.math.math_gen import math_datasets
    from .datasets.cmmlu.cmmlu_gen import cmmlu_datasets

datasets = (
        # aime2025_datasets +
        LCB_datasets +
        mmlu_pro_datasets +
        math_datasets +
        cmmlu_datasets
)

internlm_url = "http://v2.open.venus.oa.com/llmproxy"
internlm_api_key = "uiiMumeqx2RleTVgQwE3U6ZY@4133"

# done âœ…
# gpt_4o_mini = [
#     dict(
#         abbr="gpt-4o-mini",
#         type=OpenAISDK,
#         path="gpt-4o-mini",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key=internlm_api_key,
#         openai_api_base=internlm_url,
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=16384,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=16384,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=1,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = gpt_4o_mini

# done âœ…
# gemini_2_flash = [
#     dict(
#         abbr="gemini-2.0-flash",
#         type=OpenAISDK,
#         path="gemini-2.0-flash",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key=internlm_api_key,
#         openai_api_base=internlm_url,
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=8192,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=16384,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=1,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = gemini_2_flash

# done âœ…
# gpt_4_1 = [
#     dict(
#         abbr="GPT-4.1",
#         type=OpenAISDK,
#         path="gpt-4.1",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key=internlm_api_key,
#         openai_api_base=internlm_url,
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=16384,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=16384,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=20,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = gpt_4_1

# done âœ…
# qwen3_30b_fp8 = [
#     dict(
#         abbr="Qwen3-30B-A3B-FP8",
#         type=OpenAISDK,
#         path="Qwen/Qwen3-30B-A3B-FP8",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key="EMPTY",
#         openai_api_base="http://101.42.115.58:8000/v1/",
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=40960,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=40960,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=16,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = qwen3_30b_fp8

# done âœ…
# openai_o4_mini = [
#     dict(
#         abbr="openai-o4-mini",
#         type=OpenAISDK,
#         path="o4-mini",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key=internlm_api_key,
#         openai_api_base=internlm_url,
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=16384,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=16384,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=1,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=20,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = openai_o4_mini

# doing ğŸš€
deepseek_r1 = [
    dict(
        abbr="deepseek-r1",
        type=OpenAISDK,
        path="deepseek-r1-local-II",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
        key=internlm_api_key,
        openai_api_base=internlm_url,
        rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
        query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
        max_out_len=40960,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
        max_seq_len=40960,                   # æœ€å¤§è¾“å…¥é•¿åº¦
        temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
        batch_size=1,                       # æ‰¹å¤„ç†å¤§å°
        retry=3,                            # é‡è¯•æ¬¡æ•°
        run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
    )
]
models = deepseek_r1

# no start âŒ
# qwen3_235b_a22b_fp8 = [
#     dict(
#         abbr="Qwen3-235B-A22B-FP8",
#         type=OpenAISDK,
#         path="qwen3-235b-a22b-fp8",               # è¯·æ±‚æœåŠ¡æ—¶çš„ model name
#         key=internlm_api_key,
#         openai_api_base=internlm_url,
#         rpm_verbose=True,                   # æ˜¯å¦æ‰“å°è¯·æ±‚é€Ÿç‡
#         query_per_second=0.16,              # æœåŠ¡è¯·æ±‚é€Ÿç‡
#         max_out_len=40960,                   # æœ€å¤§è¾“å‡ºé•¿åº¦
#         max_seq_len=40960,                   # æœ€å¤§è¾“å…¥é•¿åº¦
#         temperature=0.7,                   # ç”Ÿæˆæ¸©åº¦
#         batch_size=1,                       # æ‰¹å¤„ç†å¤§å°
#         retry=3,                            # é‡è¯•æ¬¡æ•°
#         run_cfg=dict(num_gpus=0),                # èµ„æºéœ€æ±‚ï¼ˆä¸éœ€è¦ GPUï¼‰
#     )
# ]
# models = qwen3_235b_a22b_fp8

work_dir = 'outputs/default/'
station_path = "/Users/zhangyuehua/Desktop/opencompass-result"

"""
python run.py opencompass/configs/llm_eval_standard_benchmark.py --debug -r 20250503_222944 -l --read-from-station
"""